{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MshQh53NrVkr"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install ptflops\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NnWw-4FrcIC",
        "outputId": "0e4d056b-7ffa-4179-9d99-7217cc2ce39e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "model_name = \"EleutherAI/pythia-160m\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).cuda()\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPZvsjywMqIh",
        "outputId": "c0aba14d-9846-4413-a78d-df3cb8d3b964"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'label'],\n",
              "    num_rows: 76\n",
              "})"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "model.resize_token_embeddings((len(tokenizer) + 7) // 8 * 8)  # Ridimensiona a un multiplo di 8\n",
        "\n",
        "dataset = load_dataset(\"ag_news\", split=\"test[:1%]\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvU4Ehc7Mzb1"
      },
      "outputs": [],
      "source": [
        "def preprocess(example):\n",
        "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "dataset = dataset.map(preprocess, batched=True)\n",
        "\n",
        "input_ids_list = [torch.tensor(item['input_ids']) for item in dataset]\n",
        "input_ids = torch.stack(input_ids_list).cuda()\n",
        "attention_masks = torch.stack([torch.tensor(item['attention_mask']) for item in dataset]).cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzSDgBwzbn7D"
      },
      "source": [
        "if we want to generate one output at a time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaKqkHQMkxBo",
        "outputId": "34f64beb-f551-48be-f8ea-7fb706ce282e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Text: Ky. Company Wins Grant to Study Peptides (AP) AP - A company founded by a chemistry researcher at the University of Louisville won a grant to develop a method of producing better peptides, which are short chains of amino acids, the building blocks of proteins., The company is now working on a new method of producing peptides that is more efficient than the current method of producing peptides.\n",
            "\n",
            "The company is also working on a new method of producing peptides that is more efficient than the current method of producing peptides\n",
            "Execution Time: 0.6479079723358154 seconds\n"
          ]
        }
      ],
      "source": [
        "def generate_autoregressively(model, tokenizer, input_ids, attention_mask, max_new_tokens=50):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        generated_sequence = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n",
        "        generated_text = tokenizer.decode(generated_sequence[0], skip_special_tokens=True)\n",
        "        end_time = time.time()\n",
        "        execution_time = end_time - start_time\n",
        "    return generated_text, execution_time\n",
        "\n",
        "single_input_ids = input_ids[2].unsqueeze(0)  \n",
        "single_attention_mask = attention_masks[2].unsqueeze(0)  \n",
        "generated_text, single_execution_time = generate_autoregressively(model, tokenizer, single_input_ids, single_attention_mask, max_new_tokens=50)\n",
        "\n",
        "print(f\"Generated Text: {generated_text}\")\n",
        "print(f\"Execution Time: {single_execution_time} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUtW62mdbsv6"
      },
      "source": [
        "if we want to generate all the input at once, faster way of doing it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZaBBW_36kz1",
        "outputId": "c03da7e3-278b-4746-8853-f5b7e701e7b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Execution Time per Input: 0.03136988690024928 seconds\n"
          ]
        }
      ],
      "source": [
        "def generate_autoregressively(model, tokenizer, input_ids, attention_mask, max_new_tokens=50):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        generated_sequence = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n",
        "        end_time = time.time()\n",
        "        execution_time = end_time - start_time\n",
        "    return generated_sequence, execution_time\n",
        "\n",
        "# Genera il testo e misura il tempo di esecuzione\n",
        "generated_sequence, total_execution_time = generate_autoregressively(model, tokenizer, input_ids, attention_masks, max_new_tokens=50)\n",
        "\n",
        "# Calcola il tempo medio di esecuzione per input\n",
        "avg_execution_time = total_execution_time / input_ids.size(0)\n",
        "print(f\"Average Execution Time per Input: {avg_execution_time} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmjUM8okbw7s"
      },
      "source": [
        "code with also the flops, it seem to work correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oj6_zIZkC82r"
      },
      "outputs": [],
      "source": [
        "!pip install deepspeed\n",
        "import deepspeed\n",
        "\n",
        "def get_flops(model, input_ids, attention_mask, max_new_tokens=50):\n",
        "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
        "        with record_function(\"model_inference\"):\n",
        "            model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    flops = sum(evt.cpu_time_total for evt in prof.key_averages() if evt.key == \"model_inference\")\n",
        "    return flops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "I4uPhnkKEhau"
      },
      "outputs": [],
      "source": [
        "def generate_and_profile(model, tokenizer, input_ids, attention_mask, max_new_tokens=50):\n",
        "    model.eval()\n",
        "    generated_sequences = []\n",
        "    total_times = []\n",
        "    flops_list = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(input_ids.size(0)):\n",
        "            start_time = time.time()\n",
        "            input_sequence = input_ids[i:i+1]\n",
        "            mask = attention_mask[i:i+1]\n",
        "            generated_sequence = model.generate(\n",
        "                input_sequence,\n",
        "                attention_mask=mask,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "            generated_text = tokenizer.decode(generated_sequence[0], skip_special_tokens=True)\n",
        "            end_time = time.time()\n",
        "            generated_sequences.append(generated_text)\n",
        "            execution_time = end_time - start_time\n",
        "            total_times.append(execution_time)\n",
        "\n",
        "            # Calcola i FLOPs per il singolo input\n",
        "            flops = get_flops(model, input_sequence, mask, max_new_tokens)\n",
        "            flops_list.append(flops)\n",
        "\n",
        "    return generated_sequences, total_times, flops_list\n",
        "\n",
        "# Genera i testi e calcola i tempi di esecuzione e i FLOPs per l'intero dataset\n",
        "generated_texts, total_times, flops_list = generate_and_profile(model, tokenizer, input_ids, attention_masks, max_new_tokens=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWFW1j07b_o5",
        "outputId": "c10db7be-9575-47a1-f78a-06abccd6a7c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.35168942338541936\n",
            "545270.052631579\n"
          ]
        }
      ],
      "source": [
        "print(np.mean(total_times))\n",
        "print(np.mean(flops_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG2iI9uKb2bq"
      },
      "source": [
        "fastest implementation to iterate over the examples and retrieve text and time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ll8PUXTUkIOM"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_autoregressively(model, tokenizer, input_ids, attention_mask, max_new_tokens=20):\n",
        "    model.eval()\n",
        "    generated_sequences = []\n",
        "    total_times = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(input_ids.size(0)):\n",
        "            start_time = time.time()\n",
        "            input_sequence = input_ids[i:i+1]\n",
        "            mask = attention_mask[i:i+1]\n",
        "            generated_sequence = model.generate(\n",
        "                input_sequence,\n",
        "                attention_mask=mask,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "            generated_text = tokenizer.decode(generated_sequence[0], skip_special_tokens=True)\n",
        "            end_time = time.time()\n",
        "            generated_sequences.append(generated_text)\n",
        "            execution_time = end_time - start_time\n",
        "            total_times.append(execution_time)\n",
        "    return generated_sequences, total_times\n",
        "\n",
        "# Genera i testi per l'intero dataset\n",
        "generated_texts, times = generate_autoregressively(model, tokenizer, input_ids, attention_masks, max_new_tokens=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuUVPOc2DXBT",
        "outputId": "00aec08e-4eb2-4c39-ea05-df68594db96e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.3361828327178955"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.mean(times)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1LeRtyYED9r",
        "outputId": "323cbb09-0b45-4b91-b6b3-23c09f1b35c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPTNeoXForCausalLM(\n",
              "  (gpt_neox): GPTNeoXModel(\n",
              "    (embed_in): Embedding(50280, 768)\n",
              "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-11): 12 x GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (embed_out): Linear(in_features=768, out_features=50280, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZeNenSQfKQ0"
      },
      "source": [
        "One text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zj0Q1Ok-fLr-",
        "outputId": "cdf1da9e-e7a1-4b7f-ae94-9582fbe5d072"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['Hello, I am Angelo, I am from Italy and, I am from the United States. I am a student of the University of California, Berkeley.'],\n",
              " [0.5370926856994629])"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example = \"Hello, I am Angelo, I am from Italy and\"\n",
        "input = tokenizer(example, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "input_ids = input['input_ids'].cuda()\n",
        "attention_masks = input['attention_mask'].cuda()\n",
        "generate_autoregressively(model, tokenizer, input_ids, attention_masks, max_new_tokens=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byQWmM5deh97"
      },
      "source": [
        "Top-K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuiNzcOtjyNA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
