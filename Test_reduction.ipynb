{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, GPT2Model, GPT2PreTrainedModel\n",
    "from modeling_topK_gpt2 import CustomGPT2Attention, CustomGPT2Block, CustomGPT2Model, CustomGPT2LMHeadModel\n",
    "from typing import Optional, Tuple, Union\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate text\n",
    "def generate_text(model, tokenizer, input_text, max_length=100):\n",
    "    # tokenize the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
    "\n",
    "    # generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_length=max_length, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: FineWeb Technical Report was released! Here is how the Hugging Face team created FineWeb, the best open-source dataset:\n",
      "\n",
      "1. Collect Raw Data: Use CommonCrawl as the starting point, 96 CommonCrawl snapshots were used for FineWeb.\n",
      "\n",
      "2. Url Filtering: Apply URL filtering using a blocklist to remove explicit and malicious content\n",
      "\n",
      "3. Text Extraction: Use the trafilatura to extract text from raw HTML of the WARC files\n",
      "\n",
      "4. Base Filtering (Language & Gopher): Use a fastText language classifier to keep only English text and apply quality and repetition filters from MassiveText (Gopher).\n",
      "\n",
      "4. Deduplicating the Data: Use MinHash deduplication to deduplicate each dump individually. No cross-deduplication across dumps.\n",
      "\n",
      "5. Additional Quality Filtering: Apply C4 filters (except Punctuation) + 3 additional new filters for Punctuation, Line duplicates, and Short Lines.\n",
      "\n",
      "6. PII Removal: Replace Emails and IP addresses from the dataset\n",
      "\n",
      "During the whole process the team ran hundreds of ablations to evaluate performance against other open datasets.\n",
      "\n",
      "\n",
      "The team has been working on the project for a while now and we are very excited to see what the future holds.\n",
      "\n",
      "\n",
      "We are also looking forward to the future of the project.\n",
      "\n",
      "\n",
      "We are looking forward to your feedback and to the future of the project.\n",
      "\n",
      "\n",
      "Thank you for your continued support.\n",
      "\n",
      "\n",
      "- The Team\n",
      "49.03537106513977\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"  \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "config = GPT2Config.from_pretrained(model_name)\n",
    "custom_model = CustomGPT2LMHeadModel.from_pretrained(model_name, config=config, k_percent=[0.0, 0.0, 0.2, 0.2, 0.2, 0.15, 0.2, 0.2, 0.2, 0.2, 0.0, 0.05],\n",
    "                                                     layers_to_prune=[0, 1, 2,3, 4, 5, 6,7,8,9,10,11])\n",
    "custom_model.to('cuda')  \n",
    "generated_text = \"\"\"FineWeb Technical Report was released! Here is how the Hugging Face team created FineWeb, the best open-source dataset:\n",
    "\n",
    "1. Collect Raw Data: Use CommonCrawl as the starting point, 96 CommonCrawl snapshots were used for FineWeb.\n",
    "\n",
    "2. Url Filtering: Apply URL filtering using a blocklist to remove explicit and malicious content\n",
    "\n",
    "3. Text Extraction: Use the trafilatura to extract text from raw HTML of the WARC files\n",
    "\n",
    "4. Base Filtering (Language & Gopher): Use a fastText language classifier to keep only English text and apply quality and repetition filters from MassiveText (Gopher).\n",
    "\n",
    "4. Deduplicating the Data: Use MinHash deduplication to deduplicate each dump individually. No cross-deduplication across dumps.\n",
    "\n",
    "5. Additional Quality Filtering: Apply C4 filters (except Punctuation) + 3 additional new filters for Punctuation, Line duplicates, and Short Lines.\n",
    "\n",
    "6. PII Removal: Replace Emails and IP addresses from the dataset\n",
    "\n",
    "During the whole process the team ran hundreds of ablations to evaluate performance against other open datasets.\"\"\"\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "  \n",
    "  #print(len(tokenizer(generated_text)[\"input_ids\"]))\n",
    "  generated_text = generate_text(custom_model, tokenizer, generated_text, max_length=len(tokenizer(generated_text)[\"input_ids\"])+1)\n",
    "end = time.time()\n",
    "print(\"Generated Text:\", generated_text)\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: FineWeb Technical Report was released! Here is how the Hugging Face team created FineWeb, the best open-source dataset:\n",
      "\n",
      "1. Collect Raw Data: Use CommonCrawl as the starting point, 96 CommonCrawl snapshots were used for FineWeb.\n",
      "\n",
      "2. Url Filtering: Apply URL filtering using a blocklist to remove explicit and malicious content\n",
      "\n",
      "3. Text Extraction: Use the trafilatura to extract text from raw HTML of the WARC files\n",
      "\n",
      "4. Base Filtering (Language & Gopher): Use a fastText language classifier to keep only English text and apply quality and repetition filters from MassiveText (Gopher).\n",
      "\n",
      "4. Deduplicating the Data: Use MinHash deduplication to deduplicate each dump individually. No cross-deduplication across dumps.\n",
      "\n",
      "5. Additional Quality Filtering: Apply C4 filters (except Punctuation) + 3 additional new filters for Punctuation, Line duplicates, and Short Lines.\n",
      "\n",
      "6. PII Removal: Replace Emails and IP addresses from the dataset\n",
      "\n",
      "During the whole process the team ran hundreds of ablations to evaluate performance against other open datasets.\n",
      "\n",
      "\n",
      "The team also used the following tools to analyze the data:\n",
      "\n",
      "\n",
      "1. The Open Data Project: The Open Data Project is a project of the Open Data Foundation (ODF). The Open Data Project is a collaborative project between the Open Data Foundation and the Open Data Foundation. The Open Data Project is a collaborative project between the Open Data Foundation and the Open Data Foundation. The Open Data Project is a collaborative project between the Open Data Foundation and the Open Data Foundation. The Open\n",
      "118.52756428718567\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"  \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = CustomGPT2LMHeadModel.from_pretrained(model_name, config=config, k_percent=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                                                     layers_to_prune=[0, 1, 2,3, 4, 5, 6,7,8,9,10,11])\n",
    "model.to('cuda')\n",
    "\n",
    "generated_text = \"\"\"FineWeb Technical Report was released! Here is how the Hugging Face team created FineWeb, the best open-source dataset:\n",
    "\n",
    "1. Collect Raw Data: Use CommonCrawl as the starting point, 96 CommonCrawl snapshots were used for FineWeb.\n",
    "\n",
    "2. Url Filtering: Apply URL filtering using a blocklist to remove explicit and malicious content\n",
    "\n",
    "3. Text Extraction: Use the trafilatura to extract text from raw HTML of the WARC files\n",
    "\n",
    "4. Base Filtering (Language & Gopher): Use a fastText language classifier to keep only English text and apply quality and repetition filters from MassiveText (Gopher).\n",
    "\n",
    "4. Deduplicating the Data: Use MinHash deduplication to deduplicate each dump individually. No cross-deduplication across dumps.\n",
    "\n",
    "5. Additional Quality Filtering: Apply C4 filters (except Punctuation) + 3 additional new filters for Punctuation, Line duplicates, and Short Lines.\n",
    "\n",
    "6. PII Removal: Replace Emails and IP addresses from the dataset\n",
    "\n",
    "During the whole process the team ran hundreds of ablations to evaluate performance against other open datasets.\"\"\"\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "\n",
    "  #print(len(tokenizer(generated_text)[\"input_ids\"]))\n",
    "  generated_text = generate_text(model, tokenizer, generated_text, max_length=len(tokenizer(generated_text)[\"input_ids\"])+1)\n",
    "end = time.time()\n",
    "print(\"Generated Text:\", generated_text)\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: FineWeb Technical Report was released! Here is how the Hugging Face team created FineWeb, the best open-source dataset:\n",
      "\n",
      "1. Collect Raw Data: Use CommonCrawl as the starting point, 96 CommonCrawl snapshots were used for FineWeb.\n",
      "\n",
      "2. Url Filtering: Apply URL filtering using a blocklist to remove explicit and malicious content\n",
      "\n",
      "3. Text Extraction: Use the trafilatura to extract text from raw HTML of the WARC files\n",
      "\n",
      "4. Base Filtering (Language & Gopher): Use a fastText language classifier to keep only English text and apply quality and repetition filters from MassiveText (Gopher).\n",
      "\n",
      "4. Deduplicating the Data: Use MinHash deduplication to deduplicate each dump individually. No cross-deduplication across dumps.\n",
      "\n",
      "5. Additional Quality Filtering: Apply C4 filters (except Punctuation) + 3 additional new filters for Punctuation, Line duplicates, and Short Lines.\n",
      "\n",
      "6. PII Removal: Replace Emails and IP addresses from the dataset\n",
      "\n",
      "During the whole process the team ran hundreds of ablations to evaluate performance against other open datasets.\n",
      "\n",
      "\n",
      "The team also used the following tools to analyze the data:\n",
      "\n",
      "\n",
      "1. The Open Data Project: The Open Data Project is a project of the Open Data Foundation (ODF). The Open Data Project is a collaborative project between the Open Data Foundation and the Open Data Foundation. The Open Data Project is a collaborative project between the Open Data Foundation and the Open Data Foundation. The Open Data Project is a collaborative project between the Open Data Foundation and the Open Data Foundation. The Open\n",
      "114.06124639511108\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"  \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, config=config)\n",
    "model.to('cuda')\n",
    "\n",
    "generated_text = \"\"\"FineWeb Technical Report was released! Here is how the Hugging Face team created FineWeb, the best open-source dataset:\n",
    "\n",
    "1. Collect Raw Data: Use CommonCrawl as the starting point, 96 CommonCrawl snapshots were used for FineWeb.\n",
    "\n",
    "2. Url Filtering: Apply URL filtering using a blocklist to remove explicit and malicious content\n",
    "\n",
    "3. Text Extraction: Use the trafilatura to extract text from raw HTML of the WARC files\n",
    "\n",
    "4. Base Filtering (Language & Gopher): Use a fastText language classifier to keep only English text and apply quality and repetition filters from MassiveText (Gopher).\n",
    "\n",
    "4. Deduplicating the Data: Use MinHash deduplication to deduplicate each dump individually. No cross-deduplication across dumps.\n",
    "\n",
    "5. Additional Quality Filtering: Apply C4 filters (except Punctuation) + 3 additional new filters for Punctuation, Line duplicates, and Short Lines.\n",
    "\n",
    "6. PII Removal: Replace Emails and IP addresses from the dataset\n",
    "\n",
    "During the whole process the team ran hundreds of ablations to evaluate performance against other open datasets.\"\"\"\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "\n",
    "  #print(len(tokenizer(generated_text)[\"input_ids\"]))\n",
    "  generated_text = generate_text(model, tokenizer, generated_text, max_length=len(tokenizer(generated_text)[\"input_ids\"])+1)\n",
    "end = time.time()\n",
    "print(\"Generated Text:\", generated_text)\n",
    "print(end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
