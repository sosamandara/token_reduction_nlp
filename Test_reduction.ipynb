{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, GPT2Model, GPT2PreTrainedModel\n",
    "from modeling_topK_gpt2 import CustomGPT2Attention, CustomGPT2Block, CustomGPT2Model, CustomGPT2LMHeadModel\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Union\n",
    "import time\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate text\n",
    "def generate_text(model, tokenizer, input_text, max_length=100):\n",
    "    # tokenize the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
    "\n",
    "    # generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_length=max_length, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: FineWeb Technical Report was released! Here is how the Hugging Face team created FineWeb, the best open-source dataset:\n",
      "\n",
      "1. Collect Raw Data: Use CommonCrawl as the starting point, 96 CommonCrawl snapshots were used for FineWeb.\n",
      "\n",
      "2. Url Filtering: Apply URL filtering using a blocklist to remove explicit and malicious content\n",
      "\n",
      "3. Text Extraction: Use the trafilatura to extract text from raw HTML of the WARC files\n",
      "\n",
      "4. Base Filtering (Language & Gopher): Use a fastText language classifier to keep only English text and apply quality and repetition filters from MassiveText (Gopher).\n",
      "\n",
      "4. Deduplicating the Data: Use MinHash deduplication to deduplicate each dump individually. No cross-deduplication across dumps.\n",
      "\n",
      "5. Additional Quality Filtering: Apply C4 filters (except Punctuation) + 3 additional new filters for Punctuation, Line duplicates, and Short Lines.\n",
      "\n",
      "6. PII Removal: Replace Emails and IP addresses from the dataset\n",
      "\n",
      "During the whole process the team ran hundreds of ablations to evaluate performance against other open datasets.\n",
      "\n",
      "\n",
      "The team has been working on the project for a while now and we are very excited to see what the future holds.\n",
      "\n",
      "\n",
      "We are also looking forward to the future of the project.\n",
      "\n",
      "\n",
      "We are looking forward to your feedback and to the future of the project.\n",
      "\n",
      "\n",
      "Thank you for your continued support.\n",
      "\n",
      "\n",
      "- The Team\n",
      "26.20030975341797\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"  \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "config = GPT2Config.from_pretrained(model_name)\n",
    "custom_model = CustomGPT2LMHeadModel.from_pretrained(model_name, config=config, k_percent=[0.0, 0.0, 0.2, 0.2, 0.2, 0.15, 0.2, 0.2, 0.2, 0.2, 0.0, 0.05],\n",
    "                                                     layers_to_prune=[0, 1, 2,3, 4, 5, 6,7,8,9,10,11])\n",
    "custom_model.to('cuda')  \n",
    "generated_text = \"\"\"FineWeb Technical Report was released! Here is how the Hugging Face team created FineWeb, the best open-source dataset:\n",
    "\n",
    "1. Collect Raw Data: Use CommonCrawl as the starting point, 96 CommonCrawl snapshots were used for FineWeb.\n",
    "\n",
    "2. Url Filtering: Apply URL filtering using a blocklist to remove explicit and malicious content\n",
    "\n",
    "3. Text Extraction: Use the trafilatura to extract text from raw HTML of the WARC files\n",
    "\n",
    "4. Base Filtering (Language & Gopher): Use a fastText language classifier to keep only English text and apply quality and repetition filters from MassiveText (Gopher).\n",
    "\n",
    "4. Deduplicating the Data: Use MinHash deduplication to deduplicate each dump individually. No cross-deduplication across dumps.\n",
    "\n",
    "5. Additional Quality Filtering: Apply C4 filters (except Punctuation) + 3 additional new filters for Punctuation, Line duplicates, and Short Lines.\n",
    "\n",
    "6. PII Removal: Replace Emails and IP addresses from the dataset\n",
    "\n",
    "During the whole process the team ran hundreds of ablations to evaluate performance against other open datasets.\"\"\"\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "  \n",
    "  #print(len(tokenizer(generated_text)[\"input_ids\"]))\n",
    "  generated_text = generate_text(custom_model, tokenizer, generated_text, max_length=len(tokenizer(generated_text)[\"input_ids\"])+1)\n",
    "end = time.time()\n",
    "print(\"Generated Text:\", generated_text)\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: FineWeb Technical Report was released! Here is how the Hugging Face team created FineWeb, the best open-source dataset:\n",
      "\n",
      "1. Collect Raw Data: Use CommonCrawl as the starting point, 96 CommonCrawl snapshots were used for FineWeb.\n",
      "\n",
      "2. Url Filtering: Apply URL filtering using a blocklist to remove explicit and malicious content\n",
      "\n",
      "3. Text Extraction: Use the trafilatura to extract text from raw HTML of the WARC files\n",
      "\n",
      "4. Base Filtering (Language & Gopher): Use a fastText language classifier to keep only English text and apply quality and repetition filters from MassiveText (Gopher).\n",
      "\n",
      "4. Deduplicating the Data: Use MinHash deduplication to deduplicate each dump individually. No cross-deduplication across dumps.\n",
      "\n",
      "5. Additional Quality Filtering: Apply C4 filters (except Punctuation) + 3 additional new filters for Punctuation, Line duplicates, and Short Lines.\n",
      "\n",
      "6. PII Removal: Replace Emails and IP addresses from the dataset\n",
      "\n",
      "During the whole process the team ran hundreds of ablations to evaluate performance against other open datasets.\n",
      "\n",
      "\n",
      "The team also used the following tools to analyze the data:\n",
      "\n",
      "\n",
      "1. The Open Data Project: The Open Data Project is a project of the Open Data Foundation (ODF). The Open Data Project is a collaborative project between the Open Data Foundation and the Open Data Foundation. The Open Data Project is a collaborative project between the Open Data Foundation and the Open Data Foundation. The Open Data Project is a collaborative project between the Open Data Foundation and the Open Data Foundation. The Open\n",
      "76.89719724655151\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"  \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = CustomGPT2LMHeadModel.from_pretrained(model_name, config=config, k_percent=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                                                     layers_to_prune=[0, 1, 2,3, 4, 5, 6,7,8,9,10,11])\n",
    "model.to('cuda')\n",
    "\n",
    "generated_text = \"\"\"FineWeb Technical Report was released! Here is how the Hugging Face team created FineWeb, the best open-source dataset:\n",
    "\n",
    "1. Collect Raw Data: Use CommonCrawl as the starting point, 96 CommonCrawl snapshots were used for FineWeb.\n",
    "\n",
    "2. Url Filtering: Apply URL filtering using a blocklist to remove explicit and malicious content\n",
    "\n",
    "3. Text Extraction: Use the trafilatura to extract text from raw HTML of the WARC files\n",
    "\n",
    "4. Base Filtering (Language & Gopher): Use a fastText language classifier to keep only English text and apply quality and repetition filters from MassiveText (Gopher).\n",
    "\n",
    "4. Deduplicating the Data: Use MinHash deduplication to deduplicate each dump individually. No cross-deduplication across dumps.\n",
    "\n",
    "5. Additional Quality Filtering: Apply C4 filters (except Punctuation) + 3 additional new filters for Punctuation, Line duplicates, and Short Lines.\n",
    "\n",
    "6. PII Removal: Replace Emails and IP addresses from the dataset\n",
    "\n",
    "During the whole process the team ran hundreds of ablations to evaluate performance against other open datasets.\"\"\"\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "\n",
    "  #print(len(tokenizer(generated_text)[\"input_ids\"]))\n",
    "  generated_text = generate_text(model, tokenizer, generated_text, max_length=len(tokenizer(generated_text)[\"input_ids\"])+1)\n",
    "end = time.time()\n",
    "print(\"Generated Text:\", generated_text)\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: FineWeb Technical Report was released! Here is how the Hugging Face team created FineWeb, the best open-source dataset:\n",
      "\n",
      "1. Collect Raw Data: Use CommonCrawl as the starting point, 96 CommonCrawl snapshots were used for FineWeb.\n",
      "\n",
      "2. Url Filtering: Apply URL filtering using a blocklist to remove explicit and malicious content\n",
      "\n",
      "3. Text Extraction: Use the trafilatura to extract text from raw HTML of the WARC files\n",
      "\n",
      "4. Base Filtering (Language & Gopher): Use a fastText language classifier to keep only English text and apply quality and repetition filters from MassiveText (Gopher).\n",
      "\n",
      "4. Deduplicating the Data: Use MinHash deduplication to deduplicate each dump individually. No cross-deduplication across dumps.\n",
      "\n",
      "5. Additional Quality Filtering: Apply C4 filters (except Punctuation) + 3 additional new filters for Punctuation, Line duplicates, and Short Lines.\n",
      "\n",
      "6. PII Removal: Replace Emails and IP addresses from the dataset\n",
      "\n",
      "During the whole process the team ran hundreds of ablations to evaluate performance against other open datasets.\n",
      "\n",
      "\n",
      "The team also used the following tools to analyze the data:\n",
      "\n",
      "\n",
      "1. The Open Data Project: The Open Data Project is a project of the Open Data Foundation (ODF). The Open Data Project is a collaborative project between the Open Data Foundation and the Open Data Foundation. The Open Data Project is a collaborative project between the Open Data Foundation and the Open Data Foundation. The Open Data Project is a collaborative project between the Open Data Foundation and the Open Data Foundation. The Open\n",
      "73.25113606452942\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"  \n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, config=config)\n",
    "model.to('cuda')\n",
    "\n",
    "generated_text = \"\"\"FineWeb Technical Report was released! Here is how the Hugging Face team created FineWeb, the best open-source dataset:\n",
    "\n",
    "1. Collect Raw Data: Use CommonCrawl as the starting point, 96 CommonCrawl snapshots were used for FineWeb.\n",
    "\n",
    "2. Url Filtering: Apply URL filtering using a blocklist to remove explicit and malicious content\n",
    "\n",
    "3. Text Extraction: Use the trafilatura to extract text from raw HTML of the WARC files\n",
    "\n",
    "4. Base Filtering (Language & Gopher): Use a fastText language classifier to keep only English text and apply quality and repetition filters from MassiveText (Gopher).\n",
    "\n",
    "4. Deduplicating the Data: Use MinHash deduplication to deduplicate each dump individually. No cross-deduplication across dumps.\n",
    "\n",
    "5. Additional Quality Filtering: Apply C4 filters (except Punctuation) + 3 additional new filters for Punctuation, Line duplicates, and Short Lines.\n",
    "\n",
    "6. PII Removal: Replace Emails and IP addresses from the dataset\n",
    "\n",
    "During the whole process the team ran hundreds of ablations to evaluate performance against other open datasets.\"\"\"\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "\n",
    "  #print(len(tokenizer(generated_text)[\"input_ids\"]))\n",
    "  generated_text = generate_text(model, tokenizer, generated_text, max_length=len(tokenizer(generated_text)[\"input_ids\"])+1)\n",
    "end = time.time()\n",
    "print(\"Generated Text:\", generated_text)\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ag_news\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "\n",
      "\n",
      "The Dow Jones industrial average closed down 0.9 percent at $1,919.50, while the S&P 500 closed down 0.9 percent at $1,919.50.\n",
      "\n",
      "\n",
      "The Dow Jones industrial\n",
      "6.666771173477173\n"
     ]
    }
   ],
   "source": [
    "generated_text = dataset[\"train\"][0][\"text\"]\n",
    "start = time.time()\n",
    "for i in range(50):\n",
    "\n",
    "  #print(len(tokenizer(generated_text)[\"input_ids\"]))\n",
    "  generated_text = generate_text(model, tokenizer, generated_text, max_length=len(tokenizer(generated_text)[\"input_ids\"])+1)\n",
    "end = time.time()\n",
    "print(\"Generated Text:\", generated_text)\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "\n",
      "\n",
      "The company's stock has fallen by more more than than than 10 percent in the past past year, and its stock has fallen by more than 20 percent in the past year.\n",
      "\n",
      "\n",
      "The stock has been trading at $1.\n",
      "5.529221534729004\n"
     ]
    }
   ],
   "source": [
    "generated_text = dataset[\"train\"][0][\"text\"]\n",
    "start = time.time()\n",
    "for i in range(50):\n",
    "\n",
    "  #print(len(tokenizer(generated_text)[\"input_ids\"]))\n",
    "  generated_text = generate_text(custom_model, tokenizer, generated_text, max_length=len(tokenizer(generated_text)[\"input_ids\"])+1)\n",
    "end = time.time()\n",
    "print(\"Generated Text:\", generated_text)\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
