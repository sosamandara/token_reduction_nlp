{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBFxpzoCYKbk",
        "outputId": "f4929e81-31e4-4f7c-c425-586a6913d0be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'token_reduction_nlp'...\n",
            "remote: Enumerating objects: 354, done.\u001b[K\n",
            "remote: Counting objects: 100% (354/354), done.\u001b[K\n",
            "remote: Compressing objects: 100% (303/303), done.\u001b[K\n",
            "remote: Total 354 (delta 81), reused 314 (delta 46), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (354/354), 15.12 MiB | 10.74 MiB/s, done.\n",
            "Resolving deltas: 100% (81/81), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/sosamandara/token_reduction_nlp.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcHop99BYaLH",
        "outputId": "0def4658-d84e-4f4e-b617-52bba3d7b3c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/token_reduction_nlp/notebooks\n"
          ]
        }
      ],
      "source": [
        "%cd /content/token_reduction_nlp/notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kf9v8ok_YeVn",
        "outputId": "12a61779-29be-462d-9c90-5a8a0c8bcc98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current directory: d:\\TESI\\token_reduction_nlp\\notebooks\n",
            "Project root directory: d:\\TESI\\token_reduction_nlp\n",
            "Models directory added to sys.path: True\n",
            "Src directory added to sys.path: True\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Get the current working directory\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "# Assuming the notebooks are in the notebooks directory and executed from there\n",
        "project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
        "\n",
        "# Add the models and src directories to the Python path\n",
        "models_dir = os.path.join(project_root, 'models')\n",
        "src_dir = os.path.join(project_root, 'src')\n",
        "\n",
        "sys.path.append(models_dir)\n",
        "sys.path.append(src_dir)\n",
        "\n",
        "# Verify that the paths are correctly added\n",
        "print(\"Current directory:\", current_dir)\n",
        "print(\"Project root directory:\", project_root)\n",
        "print(\"Models directory added to sys.path:\", models_dir in sys.path)\n",
        "print(\"Src directory added to sys.path:\", src_dir in sys.path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sZ7E7ChdYKbt"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Conda\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n",
            "d:\\Conda\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, AutoTokenizer, AutoModel\n",
        "from modeling_topK_gpt2 import CustomGPT2LMHeadModel\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from functions import run_generation_on_dataset, plot_averages, load_custom_model\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6-qa2GxYKbu"
      },
      "source": [
        "# some trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GtV1BasYKby",
        "outputId": "93cd99c9-2243-40ee-b6a3-8409a7b514b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------\n",
            "[0.2, 0.2, 0.19, 0.18, 0.19, 0.18, 0.18]\n",
            "----------\n",
            "[0.3, 0.3, 0.29, 0.28, 0.28, 0.28, 0.28]\n",
            "----------\n",
            "[0.39, 0.4, 0.39, 0.39, 0.38, 0.38, 0.38]\n",
            "----------\n",
            "[0.51, 0.5, 0.49, 0.48, 0.48, 0.48, 0.48]\n",
            "----------\n",
            "[0.61, 0.6, 0.59, 0.58, 0.58, 0.58, 0.58]\n",
            "----------\n",
            "[0.7, 0.7, 0.69, 0.69, 0.68, 0.68, 0.68]\n",
            "----------\n",
            "[0.8, 0.8, 0.79, 0.79, 0.78, 0.78, 0.78]\n",
            "----------\n",
            "[0.9, 0.9, 0.89, 0.89, 0.88, 0.88, 0.88]\n"
          ]
        }
      ],
      "source": [
        "def calculate_reduction(lenght, mask):\n",
        "  actual_lenght=lenght\n",
        "  for percentage in mask:\n",
        "    actual_lenght = (actual_lenght - int(actual_lenght*percentage))\n",
        "  return actual_lenght + 1\n",
        "\n",
        "window_sizes = [71, 50, 100, 200, 300, 500, 1000]\n",
        "remaining_tokens = []\n",
        "per_keep = []\n",
        "\n",
        "mask_90 = [0.0, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "mask_80 = [0.0, 0.0, 0.0, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "mask_70 = [0.0, 0.0, 0.0, 0.0, 0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "mask_60 = [0.0, 0.0, 0.0, 0.0, 0.42, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "mask_50 = [0.0, 0.0, 0.0, 0.0, 0.52, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "mask_40 = [0.0, 0.0, 0.0, 0.0, 0.62, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "mask_30 = [0.0, 0.0, 0.0, 0.0, 0.72, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "mask_20 = [0.0, 0.0, 0.0, 0.0, 0.82, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "masks = [mask_20, mask_30, mask_40, mask_50,\n",
        "         mask_60, mask_70, mask_80, mask_90,]\n",
        "mask_number = [\"16\", \"30\", \"40\", \"50\", \"60\", \"70\", \"80\", \"90\"]\n",
        "for mask in masks:\n",
        "  per_keep = []\n",
        "  for w in window_sizes:\n",
        "    #remaining_tokens.append(calculate_reduction(w, mask))\n",
        "    per_keep.append(round(calculate_reduction(w, mask)/w,2))\n",
        "    #print(remaining_tokens)\n",
        "  print(\"----------\")\n",
        "  print(per_keep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "B1SXrIMUYKb1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Conda\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "config = GPT2Config.from_pretrained(model_name)\n",
        "model_gpt = GPT2LMHeadModel.from_pretrained(model_name, config=config).to('cuda')\n",
        "\n",
        "# Layers to prune\n",
        "layers_to_prune = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
        "\n",
        "#model_id = f\"top_K_90\"\n",
        "#top_K_90 =  load_custom_model(model_name, config, mask_90, selection_method=\"top_k\", layers_to_prune=layers_to_prune)\n",
        "\n",
        "custom_models = {}\n",
        "for i, mask in enumerate(masks):\n",
        "    model_id = f\"top_k_{i * 10 + 20}\"\n",
        "    custom_models[model_id] = load_custom_model(model_name, config, mask, selection_method=\"top_k\", layers_to_prune=layers_to_prune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "cYmoWykBYKb3"
      },
      "outputs": [],
      "source": [
        "# Function to generate text and calculate FLOPs\n",
        "def generate_text(model, tokenizer, input_text, max_length=100):\n",
        "    # tokenize the input text\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
        "\n",
        "    # generate text\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(input_ids, max_length=max_length, pad_token_id=tokenizer.eos_token_id)\n",
        "    print(type(model.indices), input_ids[model.indices[0]])\n",
        "    # decode the generated text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text, model.indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBTkFv5IYKb4",
        "outputId": "72198ab3-ed04-4ca9-d3f2-1787f3d022fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0--------------The\n",
            "1-------------- problem\n",
            "2-------------- is\n",
            "3-------------- not\n",
            "4-------------- the\n",
            "5-------------- problem\n",
            "6--------------.\n",
            "7-------------- The\n",
            "8-------------- problem\n",
            "9-------------- is\n",
            "10-------------- your\n",
            "11-------------- attitude\n",
            "12-------------- about\n",
            "13-------------- the\n",
            "14-------------- problem\n",
            "15--------------.\n"
          ]
        }
      ],
      "source": [
        "generated_text = \"\"\"The problem is not the problem. The problem is your attitude about the problem.\"\"\"\n",
        "for i in range(len(tokenizer.encode(generated_text, return_tensors='pt').to('cuda')[0])):\n",
        "    print(str(i)+\"--------------\"+tokenizer.decode(tokenizer.encode(generated_text, return_tensors='pt').to('cuda')[0][i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL_L4SWUdcoZ"
      },
      "outputs": [],
      "source": [
        "The problem is not the problem . The problem is your attitude about the problem .\n",
        ".,is,problem,the,problem,not,The,is,problem,problem,the\n",
        "90,80,80,70,70,60,50,50,40,30,30\n",
        "6,2,5,4,1,3,7,9,14,8,13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 6,  2,  5,  4,  1,  3,  7,  9, 14,  8, 13]], device='cuda:0')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsFTk845dUoN"
      },
      "outputs": [],
      "source": [
        "'.': '90%',          #6\n",
        "'is': '80%',         #2\n",
        "'problem': '80%',    #5\n",
        "'the': '70%',        #4\n",
        "'problem': '70%',    #1\n",
        "'not': '60%',        #3\n",
        "'The': '50',         #7\n",
        "'is': '50',          #9\n",
        "'problem': '40',     #14\n",
        "'problem': '30',     #8\n",
        "'the': '30',         #13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KFCQI6qYKb5",
        "outputId": "5fc8ea3f-dc6b-4638-cab5-35087eac24ff"
      },
      "outputs": [],
      "source": [
        "generated_text = \"\"\"The problem is not the problem. The problem is your attitude about the problem.\"\"\"\n",
        "generated_text, indices = generate_text(top_K_30, tokenizer, generated_text, max_length=len(tokenizer(generated_text)[\"input_ids\"])+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MxOiHbHYKb6",
        "outputId": "37b447f2-163b-417e-ff83-67cf8177284b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0--------------Life\n",
            "1-------------- is\n",
            "2-------------- like\n",
            "3-------------- a\n",
            "4-------------- box\n",
            "5-------------- of\n",
            "6-------------- ch\n",
            "7--------------ocol\n",
            "8--------------ates\n",
            "9--------------,\n",
            "10-------------- you\n",
            "11-------------- never\n",
            "12-------------- know\n",
            "13-------------- what\n",
            "14-------------- you\n",
            "15--------------'re\n",
            "16-------------- gonna\n",
            "17-------------- get\n",
            "18--------------.\n"
          ]
        }
      ],
      "source": [
        "generated_text = \"\"\"Life is like a box of chocolates, you never know what you're gonna get.\"\"\"\n",
        "for i in range(len(tokenizer.encode(generated_text, return_tensors='pt').to('cuda')[0])):\n",
        "    print(str(i)+\"--------------\"+tokenizer.decode(tokenizer.encode(generated_text, return_tensors='pt').to('cuda')[0][i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxsB8D_4UQGy",
        "outputId": "750b4ea0-69b8-400c-ae24-15178358d13e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 12, 19, 19])\n",
            "torch.Size([1, 12, 19, 19])\n",
            "torch.Size([1, 12, 19, 19])\n",
            "torch.Size([1, 12, 19, 19])\n",
            "torch.Size([1, 12, 19, 19])\n",
            "tensor([[ 1,  3,  9,  5,  6,  4, 10,  8,  2,  7, 12, 16, 11]], device='cuda:0') torch.Size([1, 19])\n",
            "torch.Size([1, 12, 6, 6])\n",
            "torch.Size([1, 12, 6, 6])\n",
            "torch.Size([1, 12, 6, 6])\n",
            "torch.Size([1, 12, 6, 6])\n",
            "torch.Size([1, 12, 6, 6])\n",
            "torch.Size([1, 12, 6, 6])\n",
            "torch.Size([1, 12, 6, 6])\n"
          ]
        }
      ],
      "source": [
        "generated_text = \"\"\"Life is like a box of chocolates, you never know what you're gonna get.\"\"\"\n",
        "generated_text = generate_text(custom_models[\"top_k_30\"], tokenizer, generated_text, max_length=len(tokenizer(generated_text)[\"input_ids\"])+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxNarAMrd7S6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHpjUC4GVW76"
      },
      "outputs": [],
      "source": [
        "Life is like a box of ch ocol ates  , you never know what you 're gonna get .\n",
        "is:a:,:of:ch:box:you:ates:like:ocol:know:never:gonna\n",
        "90,90,80,80,70,70,60,50,50,40,40,30,30\n",
        "1,3,9,5,6,4,10,8,2,7,12,11,16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDVvlqwzUSlM"
      },
      "outputs": [],
      "source": [
        "'is': '90%',         #1\n",
        "'a': '90%',          #3\n",
        "',': '80%',          #9\n",
        "'of': '80%',         #5\n",
        "'ch': '70%',         #6\n",
        "'box': '70%',        #4\n",
        "'you': '60',         #10\n",
        "'ates': '50',        #8\n",
        "'like': '50',        #2\n",
        "'ocol': '40',        #7\n",
        "'know': '40',        #12\n",
        "'gonna': '30',        #16\n",
        "'never': '30',        #11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcHXPgHHYKb9",
        "outputId": "0640ff21-3796-45ed-d8bc-f779e862d25b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 12, 24, 24])\n",
            "torch.Size([1, 12, 24, 24])\n",
            "torch.Size([1, 12, 24, 24])\n",
            "torch.Size([1, 12, 24, 24])\n",
            "torch.Size([1, 12, 24, 24])\n",
            "tensor([[7, 5]], device='cuda:0') torch.Size([1, 24])\n",
            "torch.Size([1, 12, 22, 22])\n",
            "torch.Size([1, 12, 22, 22])\n",
            "torch.Size([1, 12, 22, 22])\n",
            "torch.Size([1, 12, 22, 22])\n",
            "torch.Size([1, 12, 22, 22])\n",
            "torch.Size([1, 12, 22, 22])\n",
            "torch.Size([1, 12, 22, 22])\n"
          ]
        }
      ],
      "source": [
        "generated_text = generate_text(top_K_90, tokenizer, generated_text, max_length=len(tokenizer(generated_text)[\"input_ids\"])+1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
